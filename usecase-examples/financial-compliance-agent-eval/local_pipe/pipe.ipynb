{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview – Evaluating a RAG + Web Search Agent on Amazon 10-K\n",
    "\n",
    "This notebook builds and evaluates a **hybrid agent** that answers questions about Amazon using two tools:\n",
    "\n",
    "- A **RAG pipeline over Amazon’s 2023 10-K** powered by Chroma as a vector store\n",
    "- A **live web search tool** for up-to-date information\n",
    "\n",
    "The goal is to measure how well an agent powered by a Bedrock-hosted LLM can (1) pick the right tool (RAG vs web search) and (2) produce factually correct answers against a labeled ground-truth dataset.\n",
    "\n",
    "This notebook will go over this architecture: \n",
    "![architecture](../images/mrmd.png \"architecture\")\n",
    "\n",
    "## What You Get Out of This Notebook\n",
    "\n",
    "By the end of the notebook you have:\n",
    "\n",
    "- A **working hybrid agent** that routes between RAG and web search using Bedrock + Haystack.\n",
    "- A labeled **evaluation dataset** spanning both historical and current Amazon questions.\n",
    "- Multiple **quantitative metrics**:\n",
    "  - Semantic similarity (SAS) between predicted and ground-truth answers\n",
    "  - Tool usage confusion matrix + tool selection accuracy\n",
    "  - LLM-as-a-judge factual accuracy score  \n",
    "- A rich DataFrame with:\n",
    "  - Prompts, ground truths, predictions\n",
    "  - Retrieved contexts\n",
    "  - Tool usage patterns and evaluation metrics\n",
    "\n",
    "The final note points you to the `sm_pipe.ipynb` notebook under `sm_mlflow_pipe`, which takes this evaluation workflow and **productionalizes it with SageMaker Pipelines + MLflow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "This cell installs required Python packages and initializes the environment used throughout the notebook. It ensures that all dependencies such as Torch, Diffusers, Haystack, or AWS SDKs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install requirements\n",
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.core.pipeline import Pipeline\n",
    "#from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "from pandas import DataFrame\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Chroma DB vector store\n",
    "\n",
    "\n",
    "Chroma is a lightweight, open-source vector database designed for storing and retrieving document embeddings. In this workflow, we use Chroma to persist embeddings generated from the 10-K PDF and perform semantic search over the document using vector similarity, enabling question-answering and context retrieval. If no embedding model is specified \"all-MiniLM-L6-v2\" (which produces 384-dimensional embeddings) as the default embedding model.\n",
    "\n",
    "### ChromaDB Metadata Field Explanations\n",
    "\n",
    "- **split_id** — The index of this text chunk within the full set of splits generated from the document.\n",
    "- **file_path** — The name of the original source file from which this chunk was extracted.\n",
    "- **source_id** — A unique hash/identifier representing the original document, used to group or deduplicate content.\n",
    "- **page_number** — The PDF page number where this chunk originates.\n",
    "- **split_idx_start** — The character offset in the full extracted text indicating where this chunk begins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File source\n",
    "file_names = ['../data/AMZN-2023-10k.pdf']\n",
    "\n",
    "#Folder where vector DB will exist \n",
    "document_store = ChromaDocumentStore(persist_path='../data/10k-vec-db')\n",
    "\n",
    "# Uncomment to recreate the vec store\n",
    "# pipeline = Pipeline()\n",
    "# pipeline.add_component(\"converter\", PyPDFToDocument())\n",
    "# pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "# pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"word\", split_length=150))\n",
    "# pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "# pipeline.connect(\"converter\", \"cleaner\")\n",
    "# pipeline.connect(\"cleaner\", \"splitter\")\n",
    "# pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "# pipeline.run({\"converter\": {\"sources\": file_names}})\n",
    "\n",
    "#Setup Chroma retriever\n",
    "retriever = ChromaQueryTextRetriever(document_store=document_store)\n",
    "\n",
    "#query with top k \n",
    "results = retriever.run(query=\"Who is Amazon's Senior Vice President and General Counsel?\", top_k=2)\n",
    "\n",
    "#Output parser\n",
    "docs = results[\"documents\"]\n",
    "\n",
    "def doc_to_json(doc):\n",
    "    return {\n",
    "        \"content\": doc.content,\n",
    "        \"meta\": doc.meta,\n",
    "        \"score\": doc.score\n",
    "    }\n",
    "\n",
    "pretty = json.dumps([doc_to_json(d) for d in docs], indent=2)\n",
    "print(pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Setup with RAG + Web Search Tools\n",
    "\n",
    "This cell defines an Agent where a Bedrock-hosted LLM decides **when to use retrieval vs. live web search**, based on the time period of the question.\n",
    "\n",
    "**Key Components:**\n",
    "- **AmazonBedrockChatGenerator**: The LLM backend generating responses and deciding which tool to call.\n",
    "- **`@tool` Decorators**: Wrap Python functions so the agent can call them dynamically during reasoning.\n",
    "- **Chroma-based RAG Retriever (`rag_tool`)**: Returns grounded text chunks from the **Amazon 10-K (up to 2023)**.\n",
    "- **DuckDuckGo Search (`web_search`)**: Provides **current, real-time** info for 2024+ queries.\n",
    "\n",
    "**Behavioral Logic Enforced in `system_prompt`:**\n",
    "- **Use RAG** for questions asking about **2023 or earlier**.\n",
    "- **Use Web Search** for **2024 or later** or “current” questions.\n",
    "- **Exactly one tool call per question** — **never both**.\n",
    "- **No extra reasoning shown** — final outputs are clean, factual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.agents import Agent\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockChatGenerator\n",
    "from haystack.tools import tool\n",
    "from ddgs.exceptions import DDGSException, RatelimitException\n",
    "from ddgs import DDGS\n",
    "\n",
    "@tool\n",
    "def web_search(keywords: str, region: str = \"us-en\", max_results: int = 3) -> str:\n",
    "    \"\"\"Search the web for updated information.\n",
    "    \n",
    "    Args:\n",
    "        keywords (str): The search query keywords.\n",
    "        region (str): The search region: wt-wt, us-en, uk-en, ru-ru, etc..\n",
    "        max_results (int | None): The maximum number of results to return.\n",
    "    Returns:\n",
    "        List of dictionaries with search results.\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = DDGS().text(keywords, region=region, max_results=max_results)\n",
    "        return results if results else \"No results found.\"\n",
    "    except RatelimitException:\n",
    "        return \"Rate limit reached. Please try again later.\"\n",
    "    except DDGSException as e:\n",
    "        return f\"Search error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def rag_tool(query: str) -> str:\n",
    "    \"\"\"Use this tool to get grounded information for answering users queries about 2023 and earlier\"\"\"\n",
    "    docs = retriever.run(query=query)['documents']\n",
    "    return [doc.content for doc in docs]\n",
    "\n",
    "agent = Agent(\n",
    "    chat_generator=AmazonBedrockChatGenerator(model=\"qwen.qwen3-32b-v1:0\", generation_kwargs={\"temperature\":0.1}),\n",
    "    tools=[web_search, rag_tool],\n",
    "    system_prompt=\"\"\"\n",
    "        You are a professional Amazon research agent with access to two tools:\n",
    "        1. Context retrieval tool: Contains Amazon 10-K filings data through 2023\n",
    "        2. Web search tool: For current information beyond 2023\n",
    "\n",
    "        TOOL SELECTION RULES:\n",
    "        - Use ONLY the context retrieval tool for questions about Amazon data from 2023 or earlier\n",
    "        - Use ONLY the web search tool for questions about Amazon data from 2024 or later\n",
    "        - NEVER use both tools for a single query\n",
    "        - You must call the single tool you selected based on the criteria ONCE AND ONLY ONCE\n",
    "\n",
    "        EXAMPLES FOR CONTEXT RETRIEVAL TOOL (2023 and earlier data):\n",
    "        - \"What was Amazon's revenue in 2022?\" → Use context_retrieval_tool\n",
    "        - \"Who was Amazon's CFO in 2023?\" → Use context_retrieval_tool\n",
    "        - \"What were Amazon's operating expenses in 2021?\" → Use context_retrieval_tool\n",
    "        - \"Who served on Amazon's board of directors in 2023?\" → Use context_retrieval_tool\n",
    "\n",
    "        EXAMPLES FOR WEB SEARCH TOOL (2024 and later data):\n",
    "        - \"What is Amazon's current stock price?\" → Use web_search\n",
    "        - \"What are Amazon's 2024 earnings?\" → Use web_search\n",
    "        - \"Who is Amazon's current CEO?\" → Use web_search\n",
    "        - \"What new products did Amazon launch in 2024?\" → Use web_search\n",
    "\n",
    "        DECISION LOGIC:\n",
    "        - If the question asks about historical data (2023 or earlier) → context_retrieval_tool\n",
    "        - If the question asks about current/recent data (2024 or later) → web_search\n",
    "        - If the question doesn't specify a time period but asks for \"current\" information → web_search\n",
    "\n",
    "        Give concise, factual answers without preamble. Always use exactly one tool per response.\n",
    "        \"\"\",\n",
    "    exit_conditions=[\"text\"], # <-- stops when agent starts to produce normal conversation text \n",
    "    max_agent_steps=2, # <-- one tool call + one final answer \n",
    "    raise_on_tool_invocation_failure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.warm_up() #<-- performs a lightweight, dry-run initialization so that all this setup happens before the user asks their first real question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Tool Example\n",
    "\n",
    "- A **ChatPromptBuilder** is used to generate a structured prompt containing strict instructions.\n",
    "- The system enforces:\n",
    "  - One and only one tool call  \n",
    "  - Tool selection based on year-specific logic  \n",
    "    - Questions about **2023 or earlier** → `context_retrieval_tool`  \n",
    "    - Questions about **2024 or current data** → `web_search`\n",
    "- The formatted prompt is sent to the agent, which executes the single tool call.\n",
    "- The response is printed using a helper function to visualize the messages and tool usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chat_message_printer import print_chat_messages\n",
    "from haystack.components.builders.chat_prompt_builder import ChatPromptBuilder\n",
    "\n",
    "#Setup prompt template\n",
    "def format_prompt(query):\n",
    "    template = [ChatMessage.from_user(\n",
    "        \"Using only ONE of the available tools, accurately answer the following question:\\n\\n{{question}}\\n\\n\" +\n",
    "        \"CRITICAL INSTRUCTIONS:\\n\" +\n",
    "        \"- Select EXACTLY ONE tool based on the time period criteria in your system prompt\\n\" +\n",
    "        \"- Make ONLY ONE tool call - do not break down or modify the query\\n\" +\n",
    "        \"- If the question is about 2023 or earlier Amazon data → use context_retrieval_tool\\n\" +\n",
    "        \"- If the question is about 2024+ or current Amazon data → use web_search\\n\" +\n",
    "        \"- Answer directly after your single tool call\"\n",
    "    )]\n",
    "    builder = ChatPromptBuilder(template=template, required_variables=['question'])\n",
    "    result = builder.run(question=query)\n",
    "    return result['prompt']\n",
    "#Add query to template\n",
    "prompt = format_prompt(\"Who is Amazon's current CEO?\")\n",
    "\n",
    "#Send query to agent\n",
    "response = agent.run(prompt)\n",
    "\n",
    "#Output parser\n",
    "print_chat_messages(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG tool example\n",
    "\n",
    "This cell is commented out for demo's sake, feel free to uncomment to test the RAG tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add query to template\n",
    "#prompt = format_prompt(\"Who was Amazon's Senior Vice President and General Counsel in 2023?\")\n",
    "#Send query to agent\n",
    "#response = agent.run(prompt)\n",
    "#output parser\n",
    "#print_chat_messages(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth Dataset Overview\n",
    "\n",
    "This dataset represents a collection of question–answer pairs used to evaluate the performance of the agent. Each row contains both the **query** and the **expected answer**, along with metadata describing how the ground-truth label was sourced.\n",
    "\n",
    "**Columns Explained**\n",
    "\n",
    "- **prompt** — The natural-language question that will be passed to the agent.  \n",
    "  This is the input the agent must answer using either RAG or web search.\n",
    "\n",
    "- **context** — (Optional) The extracted text from the source document used to derive the ground-truth answer.  \n",
    "  - Present for **RAG-labeled** rows.  \n",
    "  - `None` for web-search questions.\n",
    "\n",
    "- **output** — The ground-truth answer.  \n",
    "  This is the gold-standard response that the agent’s answer will be compared against.\n",
    "\n",
    "- **page** — (For RAG rows) The page number in the Amazon 10-K document where the supporting information was found.\n",
    "\n",
    "- **tool_label** — Indicates which tool should ideally be used to answer the question:  \n",
    "  - **`rag`** → Question relates to *historical Amazon data* found in the PDF.  \n",
    "  - **`web_search`** → Question relates to *current or modern information* requiring external web search.\n",
    "\n",
    "---\n",
    "\n",
    "### How We Use This Data for Evaluation\n",
    "\n",
    "For each row:\n",
    "\n",
    "1. We take the **prompt** and send it to the agent.  \n",
    "2. The agent selects a tool (`rag` or `web_search`) following system rules.  \n",
    "3. The agent returns an answer.  \n",
    "4. We compare the agent’s answer with the **output** column (ground truth).  \n",
    "5. For RAG rows, we can also check if the retrieved chunks match the **context** column.  \n",
    "6. Metrics such as accuracy, similarity scores, or nDCG are computed to determine how close the agent’s answer is to the ground truth.\n",
    "\n",
    "This process allows us to measure:\n",
    "- Tool-selection correctness  \n",
    "- Factual correctness  \n",
    "- Retrieval quality for RAG-based questions  \n",
    "- Overall agent reliability across mixed data sources  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('../data/ground_truth.json', orient='records')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers \n",
    "- Generating answers based on the ground truth prompt, takes approximately ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "answers = []\n",
    "for prompt in tqdm(df.prompt.to_list()):\n",
    "    print(prompt)\n",
    "    prompt = format_prompt(prompt)\n",
    "    answers.append(agent.run(prompt))\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Normalize Agent Answers\n",
    "\n",
    "This cell processes the raw agent responses and prepares them for evaluation:\n",
    "\n",
    "- **`raw_answers`** stores the full response object returned by the agent, including message history and tool calls.\n",
    "- **`clean_answers`** extracts only the final text answer from each response.  \n",
    "  - If the final message is missing or empty, it defaults to `\"I don't know\"` to avoid null values during evaluation.\n",
    "\n",
    "These cleaned answers will later be compared against the ground-truth outputs to measure answer correctness and overall agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data frame \n",
    "df['raw_answers'] = answers\n",
    "df['clean_answers'] = [answer['messages'][-1].text if answer['messages'][-1].text is not None else \"I don't know\" for answer in answers]\n",
    "# uncomment below if you would like to print\n",
    "#df['clean_answers'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Retrieved Documents from Agent Tool Calls\n",
    "\n",
    "This cell builds a robust parser that extracts the **retrieved context documents** returned by the agent’s `context_retrieval_tool`. Since tool responses may vary in structure (JSON strings, Python literals, dicts, lists, or custom objects), the helper functions standardize and normalize them into a clean list of `haystack.Document` objects.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **`_parse_result_payload`**  \n",
    "  Safely converts the raw tool payload into a Python object.  \n",
    "  - Tries JSON parsing  \n",
    "  - Falls back to `ast.literal_eval`  \n",
    "  - Defaults to wrapping the string as a single document\n",
    "\n",
    "- **`_coerce_to_documents`**  \n",
    "  Converts any parsed object—dicts, lists, nested structures—into a list of `Document` objects, ensuring consistent formatting of:\n",
    "  - `content` (text)  \n",
    "  - `meta` / metadata fields  \n",
    "\n",
    "- **`get_clean_docs(answer)`**  \n",
    "  Walks through all tool messages in the agent’s response and:\n",
    "  - Identifies messages originating from `context_retrieval_tool`\n",
    "  - Extracts the attached result payload\n",
    "  - Parses and normalizes it into a structured list of documents\n",
    "\n",
    "Finally, we populate a new dataframe column:\n",
    "\n",
    "- **`extracted_contexts`** — the cleaned list of retrieved documents per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full dataframe\n",
    "import json\n",
    "import ast\n",
    "from haystack import Document\n",
    "\n",
    "def _parse_result_payload(payload):\n",
    "    \"\"\"Return a Python object from payload (str|list|dict).\"\"\"\n",
    "    if isinstance(payload, (list, dict)):\n",
    "        return payload\n",
    "    if isinstance(payload, str):\n",
    "        s = payload.strip()\n",
    "        # try JSON first\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # then ast as a fallback\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            # last resort: treat the raw string as a single doc\n",
    "            return [{\"content\": s}]\n",
    "    # Unknown type -> wrap as string\n",
    "    return [{\"content\": str(payload)}]\n",
    "\n",
    "def _coerce_to_documents(obj):\n",
    "    \"\"\"Normalize various shapes to List[haystack.Document].\"\"\"\n",
    "    # If it's a dict, look for common keys\n",
    "    if isinstance(obj, dict):\n",
    "        for key in (\"documents\", \"docs\", \"results\", \"retrieved_documents\", \"retrieved_docs\"):\n",
    "            if key in obj and isinstance(obj[key], list):\n",
    "                return _coerce_to_documents(obj[key])\n",
    "        # maybe it’s a single doc-like dict\n",
    "        obj = [obj]\n",
    "\n",
    "    # Now expect a list of doc-like items\n",
    "    docs_out = []\n",
    "    if isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            if isinstance(item, Document):\n",
    "                docs_out.append(item)\n",
    "                continue\n",
    "            if isinstance(item, dict):\n",
    "                content = (\n",
    "                    item.get(\"content\")\n",
    "                    or item.get(\"text\")\n",
    "                    or item.get(\"page_content\")\n",
    "                    or \"\"\n",
    "                )\n",
    "                meta = item.get(\"meta\") or item.get(\"metadata\") or {}\n",
    "                if content is None:\n",
    "                    content = \"\"\n",
    "                docs_out.append(Document(content=content, meta=meta))\n",
    "                continue\n",
    "            # anything else -> coerce to string content\n",
    "            docs_out.append(Document(content=str(item), meta={}))\n",
    "    return docs_out\n",
    "\n",
    "def get_clean_docs(answer, target_tool_name=\"context_retrieval_tool\"):\n",
    "    \"\"\"\n",
    "    Walks tool messages in `answer['messages']` and extracts documents robustly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1) collect candidate payloads from TOOL messages\n",
    "        candidates = []\n",
    "        for msg in answer.get(\"messages\", []):\n",
    "            # Handle role checks defensively\n",
    "            role = getattr(msg, \"_role\", None)\n",
    "            role_val = getattr(role, \"value\", None) or getattr(msg, \"role\", None)\n",
    "            if str(role_val).lower() != \"tool\":\n",
    "                continue\n",
    "\n",
    "            content = getattr(msg, \"_content\", None) or getattr(msg, \"content\", None) or []\n",
    "            # content may be a list; scan for objects with .result\n",
    "            if isinstance(content, list):\n",
    "                for part in content:\n",
    "                    result = getattr(part, \"result\", None)\n",
    "                    origin = getattr(part, \"origin\", None)\n",
    "                    tool_name = getattr(origin, \"tool_name\", None)\n",
    "                    if result is None:\n",
    "                        continue\n",
    "                    if target_tool_name and tool_name == target_tool_name:\n",
    "                        candidates.append(result)\n",
    "                    else:\n",
    "                        # keep a fallback candidate in case no named tool matches\n",
    "                        candidates.append(result)\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        # 2) prefer first payload from the target tool if found, else first payload\n",
    "        payload = None\n",
    "        for c in candidates:\n",
    "            # pick the first one whose origin matched in the loop above\n",
    "            payload = c\n",
    "            break\n",
    "\n",
    "        # 3) parse payload -> python object\n",
    "        parsed = _parse_result_payload(payload)\n",
    "\n",
    "        # 4) normalize to haystack.Document list\n",
    "        return _coerce_to_documents(parsed)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# usage:\n",
    "df['extracted_contexts'] = [get_clean_docs(answer) for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment for extra information about the dataframe created\n",
    "#df.keys()\n",
    "#df.iloc[0].raw_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display data\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Answer Quality with Semantic Similarity\n",
    "\n",
    "To assess how well our retrieval-augmented generation (RAG) system answers questions, we need a metric that goes beyond exact keyword matching. Different answers may use different wording but still convey the same meaning — especially with LLMs.\n",
    "\n",
    "**Semantic Answer Similarity (SAS)** provides a **meaning-based evaluation** by embedding both the predicted answer and the ground-truth answer, and then measuring how similar those embeddings are. Higher scores indicate the answers are semantically closer.\n",
    "\n",
    "In the next cell, we use Haystack’s `SASEvaluator` to compute:\n",
    "- **Individual similarity scores** for each question\n",
    "- **An average score** that summarizes overall model performance\n",
    "\n",
    "We should be able to see that Qwen does decently when it comes to semantic similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.evaluators import SASEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "gt = df[\"output\"].to_list()\n",
    "pa = df[\"clean_answers\"].to_list()\n",
    "\n",
    "sas_evaluator = SASEvaluator()\n",
    "sas_evaluator.warm_up()\n",
    "result = sas_evaluator.run(\n",
    "    ground_truth_answers=gt,\n",
    "    predicted_answers=pa\n",
    ")\n",
    "\n",
    "# Attach scores to the dataframe\n",
    "df[\"sas_score\"] = result[\"individual_scores\"]\n",
    "\n",
    "# Build a compact view with truncated text\n",
    "def truncate(text, n=80):\n",
    "    text = str(text)\n",
    "    return text if len(text) <= n else text[:n] + \"...\"\n",
    "\n",
    "eval_view = df[[\"prompt\", \"output\", \"clean_answers\", \"sas_score\"]].copy()\n",
    "eval_view[\"prompt\"] = eval_view[\"prompt\"].apply(lambda x: truncate(x, 60))\n",
    "eval_view[\"output\"] = eval_view[\"output\"].apply(lambda x: truncate(x, 80))\n",
    "eval_view[\"clean_answers\"] = eval_view[\"clean_answers\"].apply(lambda x: truncate(x, 80))\n",
    "eval_view[\"sas_score\"] = eval_view[\"sas_score\"].round(3)\n",
    "\n",
    "display(eval_view)\n",
    "\n",
    "print(f\"\\nAverage SAS score: {result['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Analyze Tools Used per Agent Interaction\n",
    "\n",
    "This cell inspects the raw agent response to identify **which tools were invoked** during each question–answer interaction. Because a response may contain multiple tool messages, we:\n",
    "\n",
    "- Walk through all messages in the agent’s response.\n",
    "- Check for entries containing a `tool_name` attribute.\n",
    "- Normalize tool names:\n",
    "  - Any variant of `context_retrieval_*` → `rag`\n",
    "  - Any variant of `web_search_*` → `web_search`\n",
    "- Collect the unique tools used in a single interaction and join them with a `|` separator.\n",
    "\n",
    "The resulting column **`tool_used`** provides an easy way to audit whether the agent:\n",
    "- Selected the correct tool for the question,\n",
    "- Used multiple tools when it should have used only one,\n",
    "- Or made no tool calls at all.\n",
    "\n",
    "We will be able to see based on the results the Qwen does well in selecting Websearch when it should be a Websearch call, but often uses Websearch when it should be a RAG call. Theese are the nuances that should be considered when selecting a final model for your usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_combined_tools(raw_answer):\n",
    "    \"\"\"Extract all tools used in one interaction and join with |\"\"\"\n",
    "    tools_used = []\n",
    "    \n",
    "    if not raw_answer or 'messages' not in raw_answer:\n",
    "        return 'none'\n",
    "    \n",
    "    messages = raw_answer.get('messages', [])\n",
    "    \n",
    "    for message in messages:\n",
    "        content = getattr(message, '_content', []) or []\n",
    "        \n",
    "        for item in content:\n",
    "            if hasattr(item, 'tool_name'):\n",
    "                tool_name = item.tool_name\n",
    "\n",
    "                # Normalize tool names\n",
    "                if 'context_retrieval' in tool_name or 'rag_tool' in tool_name:\n",
    "                    tool_name = 'rag'\n",
    "                elif 'web_search' in tool_name:\n",
    "                    tool_name = 'web_search'\n",
    "                \n",
    "                if tool_name not in tools_used:\n",
    "                    tools_used.append(tool_name)\n",
    "    \n",
    "    return ' | '.join(tools_used) if tools_used else 'none'\n",
    "\n",
    "df['tool_used'] = df['raw_answers'].apply(extract_combined_tools)\n",
    "\n",
    "print(\"Tool usage patterns (normalized):\")\n",
    "print(df['tool_used'].value_counts())\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Confusion matrix: ground truth vs actual tool used\n",
    "tool_confusion = pd.crosstab(df['tool_label'], df['tool_used'])\n",
    "\n",
    "# 2. Plot heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    tool_confusion,\n",
    "    annot=True,            # show cell values\n",
    "    fmt='d',               # integer formatting\n",
    "    cmap='Blues',          # heatmap color scheme\n",
    "    linewidths=.5,\n",
    "    linecolor='gray',\n",
    "    cbar=True\n",
    ")\n",
    "plt.title(\"Tool Selection Confusion Matrix\", fontsize=14)\n",
    "plt.xlabel(\"Tool Used by Agent\")\n",
    "plt.ylabel(\"Ground Truth Tool Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Accuracy calculation\n",
    "correct = sum(\n",
    "    tool_confusion.loc[tool, tool] \n",
    "    for tool in tool_confusion.index \n",
    "    if tool in tool_confusion.columns\n",
    ")\n",
    "total = tool_confusion.to_numpy().sum()\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"Tool Selection Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge Evaluation (Factual Accuracy Check)\n",
    "\n",
    "This cell uses a **large language model as an evaluator** to determine whether the model’s generated answer is **factually correct** given the retrieved context. Instead of comparing answers by exact wording, we use the LLM to judge whether the statements are **supported by evidence** in the context.\n",
    "\n",
    "**What happens in this cell:**\n",
    "- We build an input payload (`content`) containing:\n",
    "  - The **question asked**\n",
    "  - The **model’s generated answer**\n",
    "  - The **retrieved context** (if available)\n",
    "- We provide the evaluator with **few-shot examples** showing how to score statements as:\n",
    "  - **1** → factually correct and supported\n",
    "  - **0** → incorrect or unsupported\n",
    "- We configure `LLMEvaluator` to call a **Bedrock-hosted Llama 3 model** in *judge mode*, with strict instructions to return a **JSON score only**, ensuring consistency and evaluability.\n",
    "- The evaluator then returns a **numerical score** indicating whether the model’s answer aligns with the evidence.\n",
    "\n",
    "**Why this matters:**\n",
    "This approach allows us to evaluate **grounded correctness**, not just similarity. It confirms whether the model is:\n",
    "- Using retrieved context properly\n",
    "- Avoiding hallucination\n",
    "- Making claims that are supported by the data\n",
    "\n",
    "We should be able to see that Qwen does around 50% in the factual accuracy check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from haystack.components.evaluators import LLMEvaluator\n",
    "from haystack_integrations.components.generators.amazon_bedrock import AmazonBedrockChatGenerator\n",
    "\n",
    "# Silence noisy logs (PromptBuilder warnings, etc.)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.ERROR)\n",
    "\n",
    "# Few-shot examples for LLM-as-a-judge\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"questions\": \"What was Amazon's revenue growth in Q2 2024?\",\n",
    "            \"contexts\": \"\"\"Amazon reported Q2 2024 net sales of $147.98 billion, representing a 10% increase compared to Q2 2023. North America segment sales were $90.03 billion (9% growth) and International segment sales were $31.66 billion (7% growth). AWS revenue reached $26.28 billion, up 19% year-over-year.\"\"\"\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"statements\": [\n",
    "                \"Amazon's Q2 2024 net sales were $147.98 billion, up 10% from the previous year.\",\n",
    "                \"The strong performance was driven primarily by Prime Day sales in June.\"\n",
    "            ],\n",
    "            \"statement_scores\": [1, 0]  # [accurate data, unsupported claim about Prime Day]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"questions\": \"Who is Amazon's current Chief Executive Officer?\",\n",
    "            \"contexts\": \"\"\"Andy Jassy serves as President and Chief Executive Officer of Amazon. He succeeded Jeff Bezos in July 2021. Prior to becoming CEO, Jassy founded and led Amazon Web Services (AWS) from its inception in 2006, serving as CEO of AWS until his promotion to company CEO.\"\"\"\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"statements\": [\n",
    "                \"Andy Jassy is Amazon's CEO, having succeeded Jeff Bezos in July 2021.\",\n",
    "                \"Before becoming CEO, he founded and led AWS since 2006 until his promotion.\"\n",
    "            ],\n",
    "            \"statement_scores\": [1, 1]  # [accurate, accurate]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"questions\": \"What are Amazon's main advertising revenue sources?\",\n",
    "            \"contexts\": \"\"\"Amazon's advertising services generated $12.77 billion in Q1 2024, primarily from sponsored products, sponsored brands, and display advertising. The majority comes from sellers promoting their products on Amazon's marketplace, with additional revenue from brands advertising across Amazon's properties and third-party sites.\"\"\"\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"statements\": [\n",
    "                \"Amazon's advertising revenue in Q1 2024 was $12.77 billion from sponsored products and display ads.\",\n",
    "                \"This represents a 24% increase from the same quarter in the previous year.\"\n",
    "            ],\n",
    "            \"statement_scores\": [1, 0]  # [accurate, unsupported percentage claim]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Configure Bedrock chat generator\n",
    "bedrock_generator = AmazonBedrockChatGenerator(\n",
    "    model=\"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "    generation_kwargs={\n",
    "        \"temperature\": 0\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create LLM-as-a-judge evaluator (progress bar disabled)\n",
    "llm_evaluator = LLMEvaluator(\n",
    "    instructions=\"\"\"Evaluate if the answer is factually correct given the context.\n",
    "    Return ONLY a valid JSON object with a score of:\n",
    "    - 1 for correct answers that match the context\n",
    "    - 0 for incorrect or unsupported answers\n",
    "    Response must be valid JSON with format: {\"score\": number}\"\"\",\n",
    "    chat_generator=bedrock_generator,\n",
    "    inputs=[(\"predicted_answers\", List[str])],\n",
    "    outputs=[\"score\"],\n",
    "    examples=examples,\n",
    "    raise_on_failure=True,\n",
    "    progress_bar=False,  # <- disable tqdm bar\n",
    ")\n",
    "\n",
    "# Build evaluation payload for ALL rows in df\n",
    "eval_items: list[Dict[str, Any]] = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    question = row[\"prompt\"]\n",
    "    answer = row[\"clean_answers\"]\n",
    "    ctx = row.get(\"context\", None)\n",
    "\n",
    "    item: Dict[str, Any] = {\n",
    "        \"inputs\": {\n",
    "            \"questions\": question,\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"statements\": [answer],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Attach context if available (handles None / NaN safely)\n",
    "    if ctx is not None and not (isinstance(ctx, float) and np.isnan(ctx)):\n",
    "        item[\"inputs\"][\"contexts\"] = ctx\n",
    "\n",
    "    eval_items.append(item)\n",
    "\n",
    "print(f\"Running LLM-as-a-judge factuality evaluation on {len(eval_items)} items...\\n\")\n",
    "\n",
    "# Run evaluator per item with 10s delay (no spammy prints)\n",
    "scores: list[Any] = []\n",
    "metas: list[Any] = []\n",
    "\n",
    "for i, item in enumerate(eval_items):\n",
    "    try:\n",
    "        results = llm_evaluator.run(predicted_answers=[item])\n",
    "        judge_result = results[\"results\"][0]\n",
    "        judge_meta = results[\"meta\"][0]\n",
    "\n",
    "        scores.append(judge_result.get(\"score\", None))\n",
    "        metas.append(judge_meta)\n",
    "    except Exception as e:\n",
    "        # Keep alignment even if something fails\n",
    "        scores.append(None)\n",
    "        metas.append(None)\n",
    "        print(f\"Warning: error on item {i+1}: {e}\")\n",
    "\n",
    "    if i < len(eval_items) - 1:\n",
    "        time.sleep(10)\n",
    "\n",
    "print(\"LLM-as-a-judge evaluation complete.\\n\")\n",
    "\n",
    "# Attach scores back to df & pretty summary\n",
    "df[\"llm_judge_score\"] = scores\n",
    "\n",
    "valid_scores = [s for s in scores if s is not None]\n",
    "if valid_scores:\n",
    "    avg_score = float(np.mean(valid_scores))\n",
    "    ones = sum(1 for s in valid_scores if s == 1)\n",
    "    zeros = sum(1 for s in valid_scores if s == 0)\n",
    "\n",
    "    # Try to pull model name from first successful meta\n",
    "    first_meta = next((m for m in metas if m is not None), {})\n",
    "    model_name = first_meta.get(\"model\", \"unknown\")\n",
    "    usage = first_meta.get(\"usage\", {})\n",
    "\n",
    "    summary = {\n",
    "        \"llm_judge_model\": model_name,\n",
    "        \"average_factuality_score\": avg_score,\n",
    "        \"num_items_evaluated\": len(valid_scores),\n",
    "        \"num_supported_answers_(score=1)\": ones,\n",
    "        \"num_unsupported_answers_(score=0)\": zeros,\n",
    "        \"example_token_usage\": usage,\n",
    "    }\n",
    "\n",
    "    print(\"LLM-as-a-Judge Evaluation Summary:\")\n",
    "    print(json.dumps(summary, indent=2))\n",
    "\n",
    "    # Score distribution print\n",
    "    print(\"\\nScore distribution:\")\n",
    "    print(pd.Series(valid_scores).value_counts().sort_index())\n",
    "else:\n",
    "    print(\"No valid scores returned from LLM evaluator.\")\n",
    "\n",
    "# Neat table of low-scoring answers\n",
    "print(\"\\nSample low-scoring answers (score = 0):\")\n",
    "low_scoring = df[df[\"llm_judge_score\"] == 0][\n",
    "    [\"prompt\", \"clean_answers\", \"llm_judge_score\"]\n",
    "].head(5)\n",
    "\n",
    "# Make long text wrap nicely in the notebook\n",
    "styled_low = (\n",
    "    low_scoring.style\n",
    "    .set_properties(\n",
    "        subset=[\"prompt\", \"clean_answers\"],\n",
    "        **{\"white-space\": \"pre-wrap\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "display(styled_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook End \n",
    "\n",
    "This is the end of the piece by piece evaluation. Next go under the \"sm_mlflow_pipe\" folder and open the \"sm_pipe.ipynb\" notebook to productionalize this agentic evaluation workflow with Sagemaker Pipelines & MLFlow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
