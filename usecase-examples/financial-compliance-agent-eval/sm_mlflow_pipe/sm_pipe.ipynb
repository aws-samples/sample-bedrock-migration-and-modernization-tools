{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfb3c0d",
   "metadata": {},
   "source": [
    "# Financial Compliance Agent Evaluation Pipeline (SageMaker Pipelines + Managed MLflow)\n",
    "\n",
    "This notebook productionalizes the financial compliance agent evaluation flow into a\n",
    "three-step Amazon SageMaker Pipeline:\n",
    "\n",
    "1. **Data Preparation**  \n",
    "   - Load ground-truth dataset from S3  \n",
    "   - Basic validation and dataset profiling  \n",
    "   - Log dataset metadata to **SageMaker managed MLflow**\n",
    "\n",
    "2. **Agent Inference**  \n",
    "   - Build the RAG + Web Search agent (Qwen on Amazon Bedrock)  \n",
    "   - Run inference over all ground-truth prompts  \n",
    "   - Extract normalized outputs (clean answers, retrieved contexts, tool usage)  \n",
    "   - Persist an evaluation-ready dataset to S3  \n",
    "   - Log inference-level metrics to MLflow\n",
    "\n",
    "3. **Evaluation & Metrics**  \n",
    "   - Compute semantic similarity (SAS)  \n",
    "   - Compute tool-selection confusion matrix + accuracy  \n",
    "   - (Optionally) run LLM-as-a-judge factuality checks  \n",
    "   - Log all metrics and artifacts to **SageMaker managed MLflow**  \n",
    "   - Enforce an `AccuracyThreshold` quality gate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956cce8-9b7e-4e16-8b98-8a9d8f3cfda5",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This cell cleans out any old versions of dependencies in our environment, and installs the libraries needed to run this pipeline. Make sure to run this cell below then restart the Kernal before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd3b047-9bac-4211-bf31-7d55b03def9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys, subprocess\n",
    "\n",
    "# Cell 1 - Imports\n",
    "subprocess.call(\n",
    "    [\n",
    "        sys.executable,\n",
    "        \"-m\",\n",
    "        \"pip\",\n",
    "        \"uninstall\",\n",
    "        \"-y\",\n",
    "        \"mlflow\",\n",
    "        \"haystack\",\n",
    "        \"haystack-ai\",\n",
    "        \"haystack-experimental\",\n",
    "        \"chroma-haystack\",\n",
    "        \"amazon-bedrock-haystack\",\n",
    "        \"sentence-transformers\",\n",
    "        \"protobuf\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2) Install from updated requirements.txt\n",
    "subprocess.check_call(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-r\", \"requirements.txt\"]\n",
    ")\n",
    "\n",
    "import haystack\n",
    "print(\"haystack-ai version:\", getattr(haystack, \"__version__\", \"unknown\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c57293-e2f5-4794-b3ff-1d963d2b7896",
   "metadata": {},
   "source": [
    "## Environment \n",
    "\n",
    "This cell will setup our envinronment, the two values you must manually enter here are:\n",
    "\n",
    "1/ Sagemaker role (if not using Sagemaker AI)\n",
    "\n",
    "2/ MLFlow tracking ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 2 - Setup\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterInteger,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.function_step import step  # @step decorator\n",
    "\n",
    "\n",
    "# Basic AWS / SageMaker setup\n",
    "session = boto3.Session()\n",
    "region = session.region_name or os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "\n",
    "# In SageMaker Studio this will resolve automatically\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    # Fallback for local dev; replace with your role ARN if needed\n",
    "    role = os.getenv(\"SAGEMAKER_ROLE_ARN\", \"YOUR SAGEMAKER EXECUTION ROLE HERE\")\n",
    "\n",
    "mlflow_arn=\"YOUR MLFLOW TRACKING SERVER ARN HERE\"\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = pipeline_session.default_bucket()\n",
    "base_job_prefix = \"financial-compliance-agent-eval\"\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Role:\", role)\n",
    "print(\"Default bucket:\", default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165eb0f-819a-4eb3-9112-a0a32b6acfdc",
   "metadata": {},
   "source": [
    "## Pipeline parameter setup \n",
    "\n",
    "here we are setting up parameters for our pipeline, ensure:\n",
    "\n",
    "1/ The S3 bucket path pointing to the ground truth data exists and the groundtruth file (in the data folder of this repo) is uploaded there.\n",
    "\n",
    "2/ You select the appropriate Bedrock model that would be powering the Agent under evaluation.\n",
    "\n",
    "3/ Accuracy threshold required for your usecase \n",
    "\n",
    "4/ Rate limit delay in seconds, so you do not encounter Bedrock rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403087a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 3 - Pipeline parameters\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger\n",
    "\n",
    "# Pipeline parameters (editable when starting the pipeline)\n",
    "\n",
    "# Input ground-truth dataset (ground_truth.json) in S3\n",
    "DataInputS3Uri = ParameterString(\n",
    "    name=\"DataInputS3Uri\",\n",
    "    default_value=f\"s3://{default_bucket}/{base_job_prefix}/data/ground_truth.json\",\n",
    ")\n",
    "\n",
    "# Base output prefix for artifacts & intermediate outputs\n",
    "BaseOutputS3Uri = ParameterString(\n",
    "    name=\"BaseOutputS3Uri\",\n",
    "    default_value=f\"s3://{default_bucket}/{base_job_prefix}/artifacts\",\n",
    ")\n",
    "\n",
    "# SageMaker managed MLflow tracking server ARN\n",
    "MLflowTrackingServerArn = ParameterString(\n",
    "    name=\"MLflowTrackingServerArn\",\n",
    "    #default_value=\"arn:aws:sagemaker:REGION:ACCOUNT_ID:mlflow-tracking-server/YOUR-ID\",\n",
    "    default_value=mlflow_user_arn\n",
    ")\n",
    "\n",
    "# MLflow experiment name\n",
    "MLflowExperimentName = ParameterString(\n",
    "    name=\"MLflowExperimentName\",\n",
    "    default_value=\"financial-compliance-agent-eval\",\n",
    ")\n",
    "\n",
    "# Bedrock model ID (Qwen or any chat model you want)\n",
    "ModelId = ParameterString(\n",
    "    name=\"ModelId\",\n",
    "    default_value=\"qwen.qwen3-32b-v1:0\",\n",
    ")\n",
    "\n",
    "# Prompt ID (if you later wire Bedrock Prompt Management; not strictly required here)\n",
    "PromptId = ParameterString(\n",
    "    name=\"PromptId\",\n",
    "    default_value=\"financial-compliance-base-prompt\",\n",
    ")\n",
    "\n",
    "# Evaluation + throttling parameters\n",
    "AccuracyThreshold = ParameterFloat(\n",
    "    name=\"AccuracyThreshold\",\n",
    "    default_value=0.8,\n",
    ")\n",
    "\n",
    "RateLimitDelaySeconds = ParameterInteger(\n",
    "    name=\"RateLimitDelaySeconds\",\n",
    "    default_value=10,\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a275679-192c-4f10-b789-3258b4f74f74",
   "metadata": {},
   "source": [
    "## Helper functions \n",
    "\n",
    "In this cell we have helper functions for the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35812a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 4 - S3 + Mlflow utilities \n",
    "import io\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _parse_s3_uri(s3_uri: str):\n",
    "    \"\"\"Split s3://bucket/key into (bucket, key).\"\"\"\n",
    "    if not s3_uri.startswith(\"s3://\"):\n",
    "        raise ValueError(f\"Invalid S3 URI: {s3_uri}\")\n",
    "    parsed = urlparse(s3_uri)\n",
    "    bucket = parsed.netloc\n",
    "    key = parsed.path.lstrip(\"/\")\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def read_json_records_from_s3(s3_uri: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a JSON 'records' file from S3 into a DataFrame.\"\"\"\n",
    "    bucket, key = _parse_s3_uri(s3_uri)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = obj[\"Body\"].read()\n",
    "    return pd.read_json(io.BytesIO(body), orient=\"records\")\n",
    "\n",
    "\n",
    "def write_json_records_to_s3(df: pd.DataFrame, s3_uri: str):\n",
    "    \"\"\"Write a DataFrame as JSON 'records' to S3.\"\"\"\n",
    "    bucket, key = _parse_s3_uri(s3_uri)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    body = df.to_json(orient=\"records\").encode(\"utf-8\")\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=body)\n",
    "\n",
    "\n",
    "def init_mlflow(tracking_server_arn: str, experiment_name: str):\n",
    "    \"\"\"\n",
    "    Connect to SageMaker managed MLflow and select/create the experiment.\n",
    "\n",
    "    Note: in the console you'll copy the MLflow tracking server ARN and pass\n",
    "    it via `MLflowTrackingServerArn` pipeline parameter.\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca30f89-3113-46b0-b9a2-04833a111227",
   "metadata": {},
   "source": [
    "## Step 1 - Data prep\n",
    "\n",
    "Here is where the actual first pipeline step is defined. Here we specify the compute that this step will run on, the requirements.txt file, the logic in the step, and the metrics that will be logged in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 5 - Step 1 Data prep\n",
    "from typing import Optional\n",
    "\n",
    "@step(\n",
    "    name=\"data-preparation\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    dependencies=\"requirements.txt\",\n",
    ")\n",
    "def data_preparation_step(\n",
    "    data_input_s3_uri: str,\n",
    "    base_output_s3_uri: str,\n",
    "    tracking_server_arn: str,\n",
    "    experiment_name: str,\n",
    "    pipeline_run_id: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Step 1: Load ground-truth dataset, validate, log to MLflow, and\n",
    "    write a normalized dataset to S3 for downstream steps.\n",
    "\n",
    "    Returns:\n",
    "        normalized_dataset_s3_uri (str): S3 URI of normalized dataset.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    init_mlflow(tracking_server_arn, experiment_name)\n",
    "\n",
    "    run_name = f\"data-prep-{pipeline_run_id}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        df = read_json_records_from_s3(data_input_s3_uri)\n",
    "\n",
    "        # Basic sanity checks / stats\n",
    "        n_rows = len(df)\n",
    "        n_rag = int((df.get(\"tool_label\") == \"rag\").sum()) if \"tool_label\" in df else 0\n",
    "        n_web = int((df.get(\"tool_label\") == \"web_search\").sum()) if \"tool_label\" in df else 0\n",
    "\n",
    "        mlflow.log_param(\"data_input_s3_uri\", data_input_s3_uri)\n",
    "        mlflow.log_metric(\"num_rows\", n_rows)\n",
    "        mlflow.log_metric(\"num_rag_rows\", n_rag)\n",
    "        mlflow.log_metric(\"num_web_search_rows\", n_web)\n",
    "\n",
    "        # Normalize/ensure expected columns exist\n",
    "        expected_cols = [\"prompt\", \"output\", \"tool_label\", \"context\", \"page\"]\n",
    "        for col in expected_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "\n",
    "        # Output location for normalized dataset\n",
    "        normalized_dataset_s3_uri = (\n",
    "            f\"{base_output_s3_uri.rstrip('/')}/data/ground_truth_normalized.json\"\n",
    "        )\n",
    "\n",
    "        write_json_records_to_s3(df, normalized_dataset_s3_uri)\n",
    "        mlflow.log_param(\"normalized_dataset_s3_uri\", normalized_dataset_s3_uri)\n",
    "\n",
    "    return normalized_dataset_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879dc60-e404-46d8-9c5e-b9a294a11508",
   "metadata": {},
   "source": [
    "## Agent + RAG helpers \n",
    "\n",
    "In this cell we are laying out the code & logic that will be used in the inference step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 â€“ Agent + RAG helpers \n",
    "\n",
    "import json\n",
    "import ast\n",
    "from typing import List, Any, Dict, Optional\n",
    "\n",
    "from haystack.core.pipeline import Pipeline as HaystackPipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "\n",
    "from haystack.components.agents import Agent\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.builders.chat_prompt_builder import ChatPromptBuilder\n",
    "from haystack_integrations.components.generators.amazon_bedrock import (\n",
    "    AmazonBedrockChatGenerator,\n",
    ")\n",
    "from haystack.tools import tool\n",
    "\n",
    "from ddgs import DDGS\n",
    "from ddgs.exceptions import DDGSException, RatelimitException\n",
    "\n",
    "from haystack import Document\n",
    "\n",
    "# Global retriever used by rag_tool (call init_chroma_retriever first)\n",
    "_retriever: Optional[ChromaQueryTextRetriever] = None\n",
    "\n",
    "def init_chroma_retriever(\n",
    "    pdf_paths: Optional[List[str]] = None,\n",
    "    persist_path: str = \"../data/10k-vec-db\",\n",
    "    recreate: bool = False,\n",
    "    split_length: int = 150,\n",
    ") -> ChromaQueryTextRetriever:\n",
    "    \"\"\"\n",
    "    Initialize (and optionally rebuild) a ChromaDocumentStore + retriever.\n",
    "\n",
    "    Args:\n",
    "        pdf_paths: List of local PDF paths used to (re)build the store.\n",
    "        persist_path: Folder where Chroma DB will be persisted.\n",
    "        recreate: If True, rebuild the store from the provided PDFs.\n",
    "        split_length: Word-level split length for DocumentSplitter.\n",
    "\n",
    "    Returns:\n",
    "        ChromaQueryTextRetriever instance.\n",
    "    \"\"\"\n",
    "    global _retriever\n",
    "\n",
    "    document_store = ChromaDocumentStore(persist_path=persist_path)\n",
    "\n",
    "    if recreate and pdf_paths:\n",
    "        pipe = HaystackPipeline()\n",
    "        pipe.add_component(\"converter\", PyPDFToDocument())\n",
    "        pipe.add_component(\"cleaner\", DocumentCleaner())\n",
    "        pipe.add_component(\n",
    "            \"splitter\",\n",
    "            DocumentSplitter(split_by=\"word\", split_length=split_length),\n",
    "        )\n",
    "        pipe.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "\n",
    "        pipe.connect(\"converter\", \"cleaner\")\n",
    "        pipe.connect(\"cleaner\", \"splitter\")\n",
    "        pipe.connect(\"splitter\", \"writer\")\n",
    "\n",
    "        pipe.run({\"converter\": {\"sources\": pdf_paths}})\n",
    "\n",
    "    _retriever = ChromaQueryTextRetriever(document_store=document_store)\n",
    "    return _retriever\n",
    "\n",
    "# Tools: web_search + rag_tool (uses global _retriever)\n",
    "@tool\n",
    "def web_search(keywords: str, region: str = \"us-en\", max_results: int = 3) -> Any:\n",
    "    \"\"\"Search the web for updated information.\n",
    "\n",
    "    Args:\n",
    "        keywords: The search query keywords.\n",
    "        region: The search region: wt-wt, us-en, uk-en, ru-ru, etc.\n",
    "        max_results: The maximum number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with search results, or an error string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = DDGS().text(keywords, region=region, max_results=max_results)\n",
    "        return results if results else \"No results found.\"\n",
    "    except RatelimitException:\n",
    "        return \"Rate limit reached. Please try again later.\"\n",
    "    except DDGSException as e:\n",
    "        return f\"Search error: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def rag_tool(query: str) -> List[str]:\n",
    "    \"\"\"Use this tool to get grounded information for answering queries\n",
    "    about Amazon (10-K data through 2023).\n",
    "\n",
    "    Returns a list of text chunks.\n",
    "    \"\"\"\n",
    "    if _retriever is None:\n",
    "        raise RuntimeError(\n",
    "            \"Chroma retriever is not initialized. \"\n",
    "            \"Call init_chroma_retriever(...) before using rag_tool.\"\n",
    "        )\n",
    "\n",
    "    docs = _retriever.run(query=query)[\"documents\"]\n",
    "    return [doc.content for doc in docs]\n",
    "\n",
    "# Agent builder (Qwen on Bedrock) with your system prompt\n",
    "_SYSTEM_PROMPT = \"\"\"\n",
    "You are a professional Amazon research agent with access to two tools:\n",
    "1. RAG context retrieval tool (`rag_tool`): Contains Amazon 10-K filings data through 2023.\n",
    "2. Web search tool (`web_search`): For current information beyond 2023.\n",
    "\n",
    "TOOL SELECTION RULES:\n",
    "- Use ONLY `rag_tool` for questions about Amazon data from 2023 or earlier.\n",
    "- Use ONLY `web_search` for questions about Amazon data from 2024 or later.\n",
    "- NEVER use both tools for a single query.\n",
    "- You must call the single tool you selected based on the criteria ONCE AND ONLY ONCE.\n",
    "\n",
    "EXAMPLES FOR RAG TOOL (2023 and earlier data):\n",
    "- \"What was Amazon's revenue in 2022?\" â†’ rag_tool\n",
    "- \"Who was Amazon's CFO in 2023?\" â†’ rag_tool\n",
    "- \"What were Amazon's operating expenses in 2021?\" â†’ rag_tool\n",
    "- \"Who served on Amazon's board of directors in 2023?\" â†’ rag_tool\n",
    "\n",
    "EXAMPLES FOR WEB SEARCH TOOL (2024 and later data):\n",
    "- \"What is Amazon's current stock price?\" â†’ web_search\n",
    "- \"What are Amazon's 2024 earnings?\" â†’ web_search\n",
    "- \"Who is Amazon's current CEO?\" â†’ web_search\n",
    "- \"What new products did Amazon launch in 2024?\" â†’ web_search\n",
    "\n",
    "DECISION LOGIC:\n",
    "- If the question asks about historical data (2023 or earlier) â†’ rag_tool.\n",
    "- If the question asks about current/recent data (2024 or later) â†’ web_search.\n",
    "- If the question doesn't specify a time period but asks for \"current\" information â†’ web_search.\n",
    "\n",
    "Give concise, factual answers without preamble. Always use exactly one tool per response.\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_financial_agent(model_id: str) -> Agent:\n",
    "    \"\"\"\n",
    "    Build and warm up the Haystack Agent using a Bedrock-hosted model.\n",
    "\n",
    "    Args:\n",
    "        model_id: Bedrock model ID (e.g., \"qwen.qwen3-32b-v1:0\").\n",
    "\n",
    "    Returns:\n",
    "        Warmed-up haystack Agent instance.\n",
    "    \"\"\"\n",
    "    chat_generator = AmazonBedrockChatGenerator(\n",
    "        model=model_id,\n",
    "        generation_kwargs={\"temperature\": 0.1},\n",
    "    )\n",
    "\n",
    "    agent = Agent(\n",
    "        chat_generator=chat_generator,\n",
    "        tools=[web_search, rag_tool],\n",
    "        system_prompt=_SYSTEM_PROMPT,\n",
    "        exit_conditions=[\"text\"],\n",
    "        max_agent_steps=2,  # one tool call + one final answer\n",
    "        raise_on_tool_invocation_failure=False,\n",
    "    )\n",
    "\n",
    "    agent.warm_up()\n",
    "    return agent\n",
    "\n",
    "# Prompt builder (ChatPromptBuilder)\n",
    "def format_prompt(query: str) -> List[ChatMessage]:\n",
    "    \"\"\"\n",
    "    Build a prompt for the Agent enforcing ONE tool call and time-based\n",
    "    tool-selection rules via the user message.\n",
    "    \"\"\"\n",
    "    template = [\n",
    "        ChatMessage.from_user(\n",
    "            \"Using only ONE of the available tools, accurately answer the \"\n",
    "            \"following question:\\n\\n{{question}}\\n\\n\"\n",
    "            \"CRITICAL INSTRUCTIONS:\\n\"\n",
    "            \"- Select EXACTLY ONE tool based on the time period criteria in your system prompt\\n\"\n",
    "            \"- Make ONLY ONE tool call - do not break down or modify the query\\n\"\n",
    "            \"- If the question is about 2023 or earlier Amazon data â†’ use rag_tool\\n\"\n",
    "            \"- If the question is about 2024+ or current Amazon data â†’ use web_search\\n\"\n",
    "            \"- Answer directly after your single tool call\"\n",
    "        )\n",
    "    ]\n",
    "    builder = ChatPromptBuilder(template=template, required_variables=[\"question\"])\n",
    "    result = builder.run(question=query)\n",
    "    return result[\"prompt\"]  # List[ChatMessage]\n",
    "\n",
    "# Tool-context extraction (get_clean_docs) â€“ adapted to rag_tool\n",
    "def _parse_result_payload(payload: Any) -> Any:\n",
    "    \"\"\"Return a Python object from payload (str|list|dict).\"\"\"\n",
    "    if isinstance(payload, (list, dict)):\n",
    "        return payload\n",
    "    if isinstance(payload, str):\n",
    "        s = payload.strip()\n",
    "        # try JSON first\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "        # then ast as a fallback\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            # last resort: treat the raw string as a single doc\n",
    "            return [{\"content\": s}]\n",
    "    # Unknown type -> wrap as string\n",
    "    return [{\"content\": str(payload)}]\n",
    "\n",
    "def _coerce_to_documents(obj: Any) -> List[Document]:\n",
    "    \"\"\"Normalize various shapes to List[haystack.Document].\"\"\"\n",
    "    # If it's a dict, look for common keys\n",
    "    if isinstance(obj, dict):\n",
    "        for key in (\"documents\", \"docs\", \"results\", \"retrieved_documents\", \"retrieved_docs\"):\n",
    "            if key in obj and isinstance(obj[key], list):\n",
    "                return _coerce_to_documents(obj[key])\n",
    "        # maybe itâ€™s a single doc-like dict\n",
    "        obj = [obj]\n",
    "\n",
    "    docs_out: List[Document] = []\n",
    "    if isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            if isinstance(item, Document):\n",
    "                docs_out.append(item)\n",
    "                continue\n",
    "            if isinstance(item, dict):\n",
    "                content = (\n",
    "                    item.get(\"content\")\n",
    "                    or item.get(\"text\")\n",
    "                    or item.get(\"page_content\")\n",
    "                    or \"\"\n",
    "                )\n",
    "                meta = item.get(\"meta\") or item.get(\"metadata\") or {}\n",
    "                if content is None:\n",
    "                    content = \"\"\n",
    "                docs_out.append(Document(content=content, meta=meta))\n",
    "                continue\n",
    "            # anything else -> coerce to string content\n",
    "            docs_out.append(Document(content=str(item), meta={}))\n",
    "    return docs_out\n",
    "\n",
    "def get_clean_docs(answer: Dict[str, Any], target_tool_name: str = \"rag_tool\") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Walks tool messages in `answer['messages']` and extracts documents\n",
    "    robustly for the given tool name (defaults to 'rag_tool').\n",
    "    \"\"\"\n",
    "    try:\n",
    "        candidates = []\n",
    "        for msg in answer.get(\"messages\", []):\n",
    "            role = getattr(msg, \"_role\", None)\n",
    "            role_val = getattr(role, \"value\", None) or getattr(msg, \"role\", None)\n",
    "            if str(role_val).lower() != \"tool\":\n",
    "                continue\n",
    "\n",
    "            content = getattr(msg, \"_content\", None) or getattr(msg, \"content\", None) or []\n",
    "            if isinstance(content, list):\n",
    "                for part in content:\n",
    "                    result = getattr(part, \"result\", None)\n",
    "                    origin = getattr(part, \"origin\", None)\n",
    "                    tool_name = getattr(origin, \"tool_name\", None)\n",
    "                    if result is None:\n",
    "                        continue\n",
    "                    if target_tool_name and tool_name == target_tool_name:\n",
    "                        candidates.append(result)\n",
    "                    else:\n",
    "                        candidates.append(result)\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        payload = candidates[0]\n",
    "        parsed = _parse_result_payload(payload)\n",
    "        return _coerce_to_documents(parsed)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Tool usage extraction â€“ normalized to 'rag' vs 'web_search'\n",
    "def extract_combined_tools(raw_answer: Dict[str, Any]) -> str:\n",
    "    \"\"\"Extract all tools used in one interaction and join with |.\"\"\"\n",
    "    tools_used: List[str] = []\n",
    "\n",
    "    if not raw_answer or \"messages\" not in raw_answer:\n",
    "        return \"none\"\n",
    "\n",
    "    messages = raw_answer.get(\"messages\", [])\n",
    "\n",
    "    for message in messages:\n",
    "        content = getattr(message, \"_content\", []) or []\n",
    "\n",
    "        for item in content:\n",
    "            if hasattr(item, \"tool_name\"):\n",
    "                tool_name = item.tool_name\n",
    "\n",
    "                # Normalize tool names\n",
    "                if \"context_retrieval\" in tool_name or \"rag_tool\" in tool_name:\n",
    "                    tool_name = \"rag\"\n",
    "                elif \"web_search\" in tool_name:\n",
    "                    tool_name = \"web_search\"\n",
    "\n",
    "                if tool_name not in tools_used:\n",
    "                    tools_used.append(tool_name)\n",
    "\n",
    "    return \" | \".join(tools_used) if tools_used else \"none\"\n",
    "\n",
    "\n",
    "print(\"In-notebook agent + RAG helpers initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16608d14-7ca0-4852-b177-debe6bd08d95",
   "metadata": {},
   "source": [
    "## Step 2 - Agent Inference\n",
    "\n",
    "In this step we will conduct agent inference against the ground dataset just like in the first notebook. We are writing the results to a dataframe and logging metrics to MLFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bebc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Inference\n",
    "@step(\n",
    "    name=\"agent-inference\",\n",
    "    instance_type=\"ml.m5.xlarge\",   # or whatever instance type you want\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    dependencies=\"requirements.txt\",\n",
    ")\n",
    "def agent_inference_step(\n",
    "    dataset_s3_uri: str,\n",
    "    model_id: str,\n",
    "    prompt_id: str,  # kept for consistency, even if unused\n",
    "    base_output_s3_uri: str,\n",
    "    tracking_server_arn: str,\n",
    "    experiment_name: str,\n",
    "    pipeline_run_id: str,\n",
    "    rate_limit_delay: int = 10,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Step 2: Run agent inference on all prompts, build an evaluation-ready\n",
    "    dataset, and write it to S3.\n",
    "\n",
    "    Returns:\n",
    "        eval_dataset_s3_uri (str): S3 URI of results with columns:\n",
    "          [prompt, output, tool_label, context, page,\n",
    "           clean_answers, extracted_contexts, tool_used, raw_answers]\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # MLflow setup\n",
    "    init_mlflow(tracking_server_arn, experiment_name)\n",
    "\n",
    "    run_name = f\"inference-{pipeline_run_id}\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        df = read_json_records_from_s3(dataset_s3_uri)\n",
    "\n",
    "        mlflow.log_param(\"dataset_s3_uri\", dataset_s3_uri)\n",
    "        mlflow.log_param(\"model_id\", model_id)\n",
    "        mlflow.log_param(\"prompt_id\", prompt_id)\n",
    "        mlflow.log_param(\"rate_limit_delay_seconds\", rate_limit_delay)\n",
    "\n",
    "        # Initialize Chroma RAG index (no rebuild by default)\n",
    "        init_chroma_retriever(\n",
    "            pdf_paths=None,      # or [\"../data/AMZN-2023-10k.pdf\"] if you want to rebuild\n",
    "            recreate=False,\n",
    "        )\n",
    "\n",
    "        # Build Bedrock-backed financial agent (Qwen + RAG + web_search)\n",
    "        agent = build_financial_agent(model_id=model_id)\n",
    "\n",
    "        # Run inference over all prompts\n",
    "        prompts = df[\"prompt\"].tolist()\n",
    "        answers = []\n",
    "\n",
    "        for p in tqdm(prompts, desc=\"Running agent inference\"):\n",
    "            try:\n",
    "                prompt_msg_list = format_prompt(p)  # List[ChatMessage]\n",
    "                res = agent.run(prompt_msg_list)\n",
    "            except Exception as e:\n",
    "                # Keep alignment even if something fails\n",
    "                res = {\"messages\": [], \"error\": str(e)}\n",
    "            answers.append(res)\n",
    "            time.sleep(rate_limit_delay)\n",
    "\n",
    "        # Attach raw answers\n",
    "        df[\"raw_answers\"] = answers\n",
    "\n",
    "        # Clean final answers (mirror your original notebook logic)\n",
    "        def get_final_text(answer):\n",
    "            try:\n",
    "                msgs = answer[\"messages\"]\n",
    "                last = msgs[-1]\n",
    "                return getattr(last, \"text\", None) or getattr(last, \"content\", None) or \"I don't know\"\n",
    "            except Exception:\n",
    "                return \"I don't know\"\n",
    "\n",
    "        df[\"clean_answers\"] = [get_final_text(a) for a in answers]\n",
    "\n",
    "        # Extract retrieved contexts for RAG\n",
    "        df[\"extracted_contexts\"] = [get_clean_docs(a) for a in answers]\n",
    "\n",
    "        # Extract tool usage\n",
    "        df[\"tool_used\"] = [extract_combined_tools(a) for a in answers]\n",
    "\n",
    "        # Basic counts for logging\n",
    "        tool_counts = df[\"tool_used\"].value_counts().to_dict()\n",
    "        for k, v in tool_counts.items():\n",
    "            mlflow.log_metric(f\"tool_used_{k}\", float(v))\n",
    "\n",
    "        # Write evaluation-ready dataset to S3\n",
    "        eval_dataset_s3_uri = (\n",
    "            f\"{base_output_s3_uri.rstrip('/')}/eval/agent_eval_dataset.json\"\n",
    "        )\n",
    "        write_json_records_to_s3(df, eval_dataset_s3_uri)\n",
    "        mlflow.log_param(\"eval_dataset_s3_uri\", eval_dataset_s3_uri)\n",
    "\n",
    "    return eval_dataset_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb1afdc-c9d0-4cb7-b756-fec35380c03c",
   "metadata": {},
   "source": [
    "## Step 3 - Evaluations \n",
    "\n",
    "In this cell we will be setting up the 3rd pipeline step for the evaluations. Here we will be evaluating the metrics such as Semantic similarity, tool selection accuracy, and LLM-as-Judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 â€“ Evaluation step (SAS, tool selection, LLM-as-judge)\n",
    "\n",
    "from typing import List, Any, Dict\n",
    "\n",
    "\n",
    "@step(\n",
    "    name=\"agent-evaluation\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    dependencies=\"requirements.txt\",\n",
    ")\n",
    "def agent_evaluation_step(\n",
    "    eval_dataset_s3_uri: str,\n",
    "    tracking_server_arn: str,\n",
    "    experiment_name: str,\n",
    "    pipeline_run_id: str,\n",
    "    accuracy_threshold: float,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Step 3: Evaluate agent performance (SAS, tool selection, LLM-as-judge, nDCG)\n",
    "    and log all metrics to MLflow.\n",
    "\n",
    "    Inputs:\n",
    "        eval_dataset_s3_uri : S3 URI produced by the inference step\n",
    "        tracking_server_arn : SageMaker MLflow tracking server ARN\n",
    "        experiment_name     : MLflow experiment name\n",
    "        pipeline_run_id     : SageMaker Pipeline run id\n",
    "        accuracy_threshold  : Min acceptable accuracy (via LLM-as-judge)\n",
    "\n",
    "    Returns:\n",
    "        eval_dataset_s3_uri (str) â€“ same dataset location (for chaining / inspection)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import mlflow\n",
    "\n",
    "    # ðŸ”‘ Disable TensorFlow so transformers / sentence-transformers\n",
    "    #     don't try to import TF (which conflicts with Studio protobuf)\n",
    "    os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "    os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "    # Now it's safe to import Haystack evaluators (which pull in sentence-transformers)\n",
    "    from haystack.components.evaluators import (\n",
    "        SASEvaluator,\n",
    "        DocumentNDCGEvaluator,\n",
    "        LLMEvaluator,\n",
    "    )\n",
    "    from haystack_integrations.components.generators.amazon_bedrock import (\n",
    "        AmazonBedrockChatGenerator,\n",
    "    )\n",
    "    from haystack import Document  # used for nDCG ground-truth docs\n",
    "\n",
    "    # ---------- MLflow setup ----------\n",
    "    init_mlflow(tracking_server_arn, experiment_name)\n",
    "    run_name = f\"evaluation-{pipeline_run_id}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.log_param(\"eval_dataset_s3_uri\", eval_dataset_s3_uri)\n",
    "        mlflow.log_param(\"accuracy_threshold\", accuracy_threshold)\n",
    "\n",
    "        # ---------- Load evaluation dataset ----------\n",
    "        df = read_json_records_from_s3(eval_dataset_s3_uri)\n",
    "\n",
    "        # Ensure required columns exist\n",
    "        required_cols = [\"prompt\", \"output\", \"clean_answers\", \"tool_label\", \"tool_used\"]\n",
    "        for c in required_cols:\n",
    "            if c not in df.columns:\n",
    "                raise ValueError(f\"Expected column '{c}' not found in eval dataset\")\n",
    "\n",
    "        # 1) Semantic Answer Similarity (SAS)\n",
    "        gt_answers = df[\"output\"].tolist()\n",
    "        pred_answers = df[\"clean_answers\"].tolist()\n",
    "\n",
    "        sas_evaluator = SASEvaluator()\n",
    "        sas_evaluator.warm_up()\n",
    "\n",
    "        sas_result = sas_evaluator.run(\n",
    "            ground_truth_answers=gt_answers,\n",
    "            predicted_answers=pred_answers,\n",
    "        )\n",
    "\n",
    "        df[\"sas_score\"] = sas_result[\"individual_scores\"]\n",
    "        avg_sas = float(sas_result[\"score\"])\n",
    "\n",
    "        mlflow.log_metric(\"sas_avg\", avg_sas)\n",
    "        mlflow.log_metric(\"sas_min\", float(np.min(df[\"sas_score\"])))\n",
    "        mlflow.log_metric(\"sas_max\", float(np.max(df[\"sas_score\"])))\n",
    "        mlflow.log_metric(\"sas_std\", float(np.std(df[\"sas_score\"])))\n",
    "\n",
    "        # 2) Tool selection accuracy (rag vs web_search)\n",
    "        tool_correct = (df[\"tool_label\"].astype(str) == df[\"tool_used\"].astype(str))\n",
    "        tool_accuracy = float(tool_correct.mean())\n",
    "        mlflow.log_metric(\"tool_selection_accuracy\", tool_accuracy)\n",
    "\n",
    "        # 3) LLM-as-a-Judge (factual accuracy)\n",
    "        # Configure Bedrock-backed judge (Llama 3 or similar model)\n",
    "        judge_generator = AmazonBedrockChatGenerator(\n",
    "            model=\"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "            generation_kwargs={\n",
    "                \"temperature\": 0.0,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        judge_instructions = \"\"\"\n",
    "        Evaluate whether the model's answer is factually correct given the ground-truth answer.\n",
    "        Score:\n",
    "        - 1 if the predicted answer is factually consistent with the ground truth.\n",
    "        - 0 if it contradicts, omits key facts, or is clearly incorrect.\n",
    "        Return ONLY a JSON object: {\"score\": 0 or 1}\n",
    "        \"\"\".strip()\n",
    "\n",
    "        llm_judge = LLMEvaluator(\n",
    "            instructions=judge_instructions,\n",
    "            chat_generator=judge_generator,\n",
    "            inputs=[(\"predicted_answers\", List[str])],\n",
    "            outputs=[\"score\"],\n",
    "            examples=[],          # ðŸ‘ˆ required positional arg in haystack-ai==2.13.0\n",
    "            raise_on_failure=False,\n",
    "            progress_bar=False,\n",
    "        )\n",
    "\n",
    "        judge_scores: List[Any] = []\n",
    "        for _, row in df.iterrows():\n",
    "            question = str(row[\"prompt\"])\n",
    "            gt = str(row[\"output\"])\n",
    "            ans = str(row[\"clean_answers\"])\n",
    "\n",
    "            item: Dict[str, Any] = {\n",
    "                \"inputs\": {\n",
    "                    \"questions\": question,\n",
    "                    # we use ground truth as a reference \"context\" here;\n",
    "                    # if you prefer, you can also add row[\"context\"]\n",
    "                    \"contexts\": gt,\n",
    "                },\n",
    "                \"outputs\": {\n",
    "                    \"statements\": [ans],\n",
    "                },\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                res = llm_judge.run(predicted_answers=[item])\n",
    "                s = res[\"results\"][0].get(\"score\", None)\n",
    "            except Exception:\n",
    "                s = None\n",
    "            judge_scores.append(s)\n",
    "\n",
    "        df[\"llm_judge_score\"] = judge_scores\n",
    "        valid_scores = [s for s in judge_scores if s is not None]\n",
    "\n",
    "        if valid_scores:\n",
    "            avg_judge = float(np.mean(valid_scores))\n",
    "            ones = sum(1 for s in valid_scores if s == 1)\n",
    "            zeros = sum(1 for s in valid_scores if s == 0)\n",
    "\n",
    "            mlflow.log_metric(\"llm_judge_avg\", avg_judge)\n",
    "            mlflow.log_metric(\"llm_judge_supported_count\", float(ones))\n",
    "            mlflow.log_metric(\"llm_judge_unsupported_count\", float(zeros))\n",
    "        else:\n",
    "            avg_judge = 0.0\n",
    "            mlflow.log_metric(\"llm_judge_avg\", 0.0)\n",
    "\n",
    "        # 5) Accuracy gate based on LLM-as-judge\n",
    "        if valid_scores and avg_judge < accuracy_threshold:\n",
    "            # Just log a flag instead of failing the pipeline\n",
    "            mlflow.log_param(\"accuracy_gate_failed\", True)\n",
    "        else:\n",
    "            mlflow.log_param(\"accuracy_gate_failed\", False)\n",
    "\n",
    "        # For now we just return the eval_dataset_s3_uri for chaining.\n",
    "        return eval_dataset_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eade0aa-5748-4c6c-afc7-2aaa683a385a",
   "metadata": {},
   "source": [
    "## Wiring cell \n",
    "\n",
    "In this cell we will wire the 3 steps together, setting up the inputs of each step, and creating the Sagemaker pipeline object.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ecc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - Wiring \n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"financial-compliance-agent-eval-pipeline\"\n",
    "\n",
    "# Wire steps together using their DelayedReturn outputs\n",
    "\n",
    "# Step 1 â€“ Data Prep\n",
    "data_prep = data_preparation_step(\n",
    "    data_input_s3_uri=DataInputS3Uri,\n",
    "    base_output_s3_uri=BaseOutputS3Uri,\n",
    "    tracking_server_arn=MLflowTrackingServerArn,\n",
    "    experiment_name=MLflowExperimentName,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    ")\n",
    "\n",
    "# Step 2 â€“ Inference (takes the *return value* of step 1 as input)\n",
    "agent_infer = agent_inference_step(\n",
    "    dataset_s3_uri=data_prep,    # <-- pass DelayedReturn directly, no .properties\n",
    "    model_id=ModelId,\n",
    "    prompt_id=PromptId,\n",
    "    base_output_s3_uri=BaseOutputS3Uri,\n",
    "    tracking_server_arn=MLflowTrackingServerArn,\n",
    "    experiment_name=MLflowExperimentName,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    rate_limit_delay=RateLimitDelaySeconds,\n",
    ")\n",
    "\n",
    "# Step 3 â€“ Evaluation (takes the *return value* of step 2 as input)\n",
    "agent_eval = agent_evaluation_step(\n",
    "    eval_dataset_s3_uri=agent_infer,   # <-- pass DelayedReturn directly\n",
    "    tracking_server_arn=MLflowTrackingServerArn,\n",
    "    experiment_name=MLflowExperimentName,\n",
    "    pipeline_run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    accuracy_threshold=AccuracyThreshold,\n",
    ")\n",
    "\n",
    "# Create SageMaker Pipeline object\n",
    "#   - only the leaf node (agent_eval) is strictly required\n",
    "#   - SageMaker infers upstream dependencies from the DelayedReturn graph\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        DataInputS3Uri,\n",
    "        BaseOutputS3Uri,\n",
    "        MLflowTrackingServerArn,\n",
    "        MLflowExperimentName,\n",
    "        ModelId,\n",
    "        PromptId,\n",
    "        AccuracyThreshold,\n",
    "        RateLimitDelaySeconds,\n",
    "    ],\n",
    "    steps=[agent_eval],   # leaf node; data_prep & agent_infer inferred automatically\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "print(\"Pipeline object created:\", pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a84f9-4494-4b63-8bcc-9a5194d1a5ee",
   "metadata": {},
   "source": [
    "## Upsert cell \n",
    "\n",
    "In this cell we will Upsert the pipeline. This step takes all of the pipeline level parameters and starts the pipeline based on the wiring we did in the previous cell. From here, you may see the execution & Diagram in the Sagemaker Studio UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0437a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell 10 - Upsert \n",
    "\n",
    "# Register / update the pipeline definition in SageMaker\n",
    "pipeline_upsert_response = pipeline.upsert(role_arn=role)\n",
    "print(\"Upsert response:\", pipeline_upsert_response)\n",
    "\n",
    "# Example: start an execution (you can also start from the SageMaker console)\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"DataInputS3Uri\": f\"s3://{default_bucket}/{base_job_prefix}/data/ground_truth.json\",\n",
    "        \"BaseOutputS3Uri\": f\"s3://{default_bucket}/{base_job_prefix}/artifacts\",\n",
    "        \"MLflowTrackingServerArn\": mlflow_user_arn,\n",
    "        \"MLflowExperimentName\": \"financial-compliance-agent-eval\",\n",
    "        \"ModelId\": \"qwen.qwen3-32b-v1:0\",\n",
    "        \"PromptId\": \"financial-compliance-base-prompt\",\n",
    "        \"AccuracyThreshold\": 0.8,\n",
    "        \"RateLimitDelaySeconds\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Started execution:\", execution.arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfef79-d76a-40c9-8753-3e7b8a2ba16a",
   "metadata": {},
   "source": [
    "## Viewing Sagemaker pipeline & Metrics in MLFlow \n",
    "\n",
    "While the pipeline is running you can check the progress by going to your Sagemaker AI Studio domain and selecting \"pipelines\"\n",
    "![studio](../images/1-studio-ui.png \"studio\")\n",
    "\n",
    "Then select your pipeline name:\n",
    "![pipeline](../images/2-pipeline-ui.png \"pipeline\")\n",
    "\n",
    "Every pipeline can have multiple runs, known as \"executions\". Select the one you would like to view, in this case the latest one. All of the metadata associated can be viewed here as well. \n",
    "![execution](../images/3-execution-ui.png \"execution\")\n",
    "\n",
    "Here we can view the pipeline steps in action, you will be able to see if a step has failed, succeeded, and view the logs either in this pane or in Cloudwatch.\n",
    "![execution](../images/4-pipeline-dag.png \"execution\")\n",
    "\n",
    "Going back to the main Sagemaker AI Studio domain page, you can make your way over to MLFlow by clicking the MLFLow app on the top left, then identifying your MLfLow tracking server. Click the 3 dots, and click \"Open MLflow\"\n",
    "![execution](../images/5-mlflow-open.png \"execution\")\n",
    "\n",
    "Under the correct pipeline & execution identifiers you will be able to see all of the metrics we have captured in MLflow:\n",
    "![execution](../images/6-mlflow-metrics.png \"execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adbc93-a939-4d5d-86c1-7324f5f6035a",
   "metadata": {},
   "source": [
    "## Notebook end\n",
    "\n",
    "This brings us to the end of the notebook. We have learned how to take the original agent evaluation workflow, and convert it to use Sagemaker pipelines for automation, and MLflow for metric tracking. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
